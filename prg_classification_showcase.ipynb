{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoqG06tc8q3T"
      },
      "source": [
        "This notebook provides code for PRG classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlmuv5hVAVZt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!unzip \"/content/drive/MyDrive/nets/prg_classification/prg_data.zip\" -d \"/content\" &> /dev/null#   ---> Train data\n",
        "#!unzip \"/content/drive/MyDrive/nets/prg_classification/whole_candidates.zip\" -d \"/content\" &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1o5hKpaAiqY"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNl-XlMmlnnp"
      },
      "source": [
        "### Visualising data and creating DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF8dHcpm4NEg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# Setup path to data folder\n",
        "image_path = Path(\"/content/\")\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "\n",
        "'''\n",
        "Initialising train, validation and train paths.\n",
        "\n",
        "prg_path = image_path / \"PRG\"\n",
        "others_path = image_path / \"others\"\n",
        "prg_path_list = list(prg_path.glob(\"*.*\"))\n",
        "others_path_list = list(others_path.glob(\"*.*\"))\n",
        "print(len(prg_path_list), len(others_path_list))\n",
        "\n",
        "PRG:71\n",
        "Others:943\n",
        "Split: 55/6/10\n",
        "       750/100/93\n",
        "'''\n",
        "\n",
        "train_dir = image_path / \"augmented_train\"\n",
        "#train_dir = image_path / \"small_train\"\n",
        "valid_dir = image_path / \"validation\"\n",
        "#valid_dir = image_path / \"extended_validation\" #(test + prg from validation)\n",
        "#valid_dir = image_path / \"small_validation\"\n",
        "#test_dir = image_path / \"test\"\n",
        "#test_dir = image_path / \"small_test\"\n",
        "test_dir = image_path / \"augmented_test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNgm0bq3QleS"
      },
      "source": [
        "## Segmentation step by step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oea1priwUHOR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "#random.seed(42)\n",
        "\n",
        "image_path_list = list(train_dir.glob(\"*/*.jpg\"))\n",
        "\n",
        "'''\n",
        "Let's visualise random image\n",
        "'''\n",
        "random_image_path = random.choice(image_path_list)\n",
        "image_class = random_image_path.parent.stem\n",
        "original_img = Image.open(random_image_path)\n",
        "original_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HedUoBeUYy6"
      },
      "outputs": [],
      "source": [
        "!pip install astropy &> /dev/null\n",
        "!pip install photutils &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1qN9QZgWz-1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img = original_img.convert(\"L\")\n",
        "#img = img[:,:,0] --> For different color channels\n",
        "img = np.array(img)\n",
        "img = img.astype(\"float64\")\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gpNAMLIUJwr"
      },
      "outputs": [],
      "source": [
        "from photutils.background import Background2D, MedianBackground\n",
        "import cv2\n",
        "\n",
        "bkg_estimator = MedianBackground()\n",
        "bkg = Background2D(img, (10, 10), filter_size=(3, 3),\n",
        "                   bkg_estimator=bkg_estimator)\n",
        "img -= bkg.background  # subtract the background\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPbYauY5YIVj"
      },
      "outputs": [],
      "source": [
        "from astropy.convolution import convolve\n",
        "from photutils.segmentation import make_2dgaussian_kernel\n",
        "threshold = 1.5 * bkg.background_rms\n",
        "kernel = make_2dgaussian_kernel(3.0, size=5)  # FWHM = 3.0\n",
        "convolved_data = convolve(img, kernel)\n",
        "plt.imshow(convolved_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLDVf2ZIYeis"
      },
      "outputs": [],
      "source": [
        "from photutils.segmentation import detect_sources\n",
        "segment_map = detect_sources(convolved_data, threshold, npixels=10)\n",
        "print(segment_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw2OL12kYmXY"
      },
      "outputs": [],
      "source": [
        "from astropy.visualization import SqrtStretch\n",
        "from astropy.visualization.mpl_normalize import ImageNormalize\n",
        "norm = ImageNormalize(stretch=SqrtStretch())\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12.5))\n",
        "ax1.imshow(img, origin='lower', cmap='Greys_r', norm=norm)\n",
        "ax1.set_title('Background-subtracted Data')\n",
        "ax2.imshow(segment_map, origin='lower', cmap=segment_map.cmap,\n",
        "           interpolation='nearest')\n",
        "ax2.set_title('Segmentation Image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW-HhcvAZI5W"
      },
      "outputs": [],
      "source": [
        "from photutils.segmentation import deblend_sources\n",
        "segm_deblend = deblend_sources(convolved_data, segment_map,\n",
        "                               npixels=10, nlevels=32, contrast=0.001,\n",
        "                               progress_bar=False)\n",
        "plt.imshow(segm_deblend, origin='lower', cmap=segment_map.cmap,\n",
        "           interpolation='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtUivekvZgR9"
      },
      "outputs": [],
      "source": [
        "print(segm_deblend.areas)\n",
        "print(np.argmax(segm_deblend.areas))\n",
        "print(segm_deblend.labels)\n",
        "print(segm_deblend.labels[np.argmax(segm_deblend.areas)])\n",
        "print(segm_deblend.get_area(segm_deblend.labels[np.argmax(segm_deblend.areas)]))\n",
        "print(segm_deblend.max_label)\n",
        "print(segm_deblend.slices[12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLjaC6p3rxFR"
      },
      "outputs": [],
      "source": [
        "center = np.array((segm_deblend.data.shape[0] // 2, segm_deblend.data.shape[1] // 2))\n",
        "candidates = []\n",
        "for n, slc in enumerate(segm_deblend.slices):\n",
        "  x, y = slc[0], slc[1]\n",
        "  slc_center = np.array([(x.start + x.stop)//2, (y.start + y.stop)//2])\n",
        "  metric = ((slc_center[0] - center[0])**2 + (slc_center[1] - center[1])**2)**(1/2)\n",
        "  if metric < 0.1*segm_deblend.data.shape[1]:\n",
        "    candidates.append(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_60j12cbrdA"
      },
      "outputs": [],
      "source": [
        "#slc = segm_deblend.slices[np.argmax(segm_deblend.areas)] --> max not suitable cause it is possible to have another bright source on the image\n",
        "slc = segm_deblend.slices[candidates[0]]\n",
        "plt.imshow(segm_deblend[slc], origin='lower', cmap=segment_map.cmap,\n",
        "           interpolation='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6QIEhxrdR7e"
      },
      "outputs": [],
      "source": [
        "#mask = np.array(segment_map.data == segment_map.labels[np.argmax(segment_map.areas)])\n",
        "#debl_mask = np.array(segm_deblend.data == segm_deblend.labels[np.argmax(segm_deblend.areas)])\n",
        "mask = np.array(segment_map.data == segment_map.labels[candidates[0]])\n",
        "debl_mask = np.array(segm_deblend.data == segm_deblend.labels[candidates[0]])\n",
        "#plt.imshow(segm_deblend.data*mask, origin='lower', cmap=segment_map.cmap,\n",
        "#           interpolation='nearest')\n",
        "\n",
        "#fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1, 6, figsize=(16, 16))\n",
        "fig, (ax1, ax2, ax3, ax5) = plt.subplots(1, 4, figsize=(16, 16))\n",
        "\n",
        "ax1.imshow(original_img)\n",
        "ax1.set_title('Original image')\n",
        "\n",
        "ax2.imshow(img, origin='lower', cmap='Greys_r', norm=norm)\n",
        "ax2.set_title('Background-subtracted Data')\n",
        "\n",
        "ax3.imshow(img*mask, origin='lower', cmap='Greys_r', norm=norm)\n",
        "ax3.set_title('Mask')\n",
        "\n",
        "#ax4.imshow(img*debl_mask, origin='lower', cmap='Greys_r', norm=norm)\n",
        "#ax4.set_title('Deblended mask')\n",
        "\n",
        "ax5.imshow(original_img*np.repeat(mask[:, :, np.newaxis], 3, axis=2))#casting 2D grayscale mask to 3D RGB mask repeating it 3 times, can check like [:,:,0], [:,:,1], [:,:,2]\n",
        "ax5.set_title('Segmented image')\n",
        "\n",
        "#ax6.imshow(original_img*np.repeat(debl_mask[:, :, np.newaxis], 3, axis=2))#casting 2D grayscale mask to 3D RGB mask repeating it 3 times, can check like [:,:,0], [:,:,1], [:,:,2]\n",
        "#ax6.set_title('Deblended original img')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"segmap_shoowcase.pdf\", dpi=400, format='pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbxNF_A4yy4"
      },
      "source": [
        "### Let's visualise our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-uMxDJr41yq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "'''\n",
        "Let's visualise random image\n",
        "'''\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# The image class is the name of the directory where the image is stored\n",
        "image_class = random_image_path.parent.stem\n",
        "img = Image.open(random_image_path)\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUIZJZWly702"
      },
      "source": [
        "### Also let's generate additional prg galaxies using augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqIWyrUuzB48"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_prg_path = test_dir / \"prg\"\n",
        "test_prg_list = list(test_prg_path.glob(\"*\"))\n",
        "print(len(test_prg_list))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "  transforms.RandomHorizontalFlip(p=0.8), # p = probability of flip, 0.5 = 50% chance\n",
        "  transforms.RandomVerticalFlip(p=0.8),\n",
        "  transforms.RandomRotation(degrees=(0, 360))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9BNA_0WGVIQ"
      },
      "outputs": [],
      "source": [
        "train_prg_path = train_dir / \"prg\"\n",
        "train_prg_list = list(train_prg_path.glob(\"*\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as4LTeht3adC"
      },
      "outputs": [],
      "source": [
        "NUM_OF_AUGM = 8\n",
        "\n",
        "for image_path in test_prg_list:\n",
        "  with Image.open(image_path) as f:\n",
        "    for i in range(NUM_OF_AUGM):\n",
        "      transformed_img = transform(f)\n",
        "      transformed_img.save(str(image_path)[:81] + f\"aug{i+1}_\" + str(image_path)[81:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHHycu_f9Ook"
      },
      "source": [
        "## Creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0G191ZD9fG4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple, List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4BRhMrc_j6N"
      },
      "outputs": [],
      "source": [
        "# Make function to find classes in target directory\n",
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "  import os\n",
        "  \"\"\"\n",
        "  Finds the class folder names in a target directory.\n",
        "\n",
        "  Assumes target directory is in standard image classification format.\n",
        "\n",
        "  Args:\n",
        "    directory (str): target directory to load classnames from.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
        "\n",
        "  \"\"\"\n",
        "  # 1. Get the class names by scanning the target directory\n",
        "  classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "  if not classes:\n",
        "    raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
        "\n",
        "  class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "  return classes, class_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcnjWcaWBUgx"
      },
      "source": [
        "### Creating custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWlhWMhMBX-O"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, targ_dir: str, transform=None) -> None:\n",
        "    self.paths = list(Path(targ_dir).glob(\"*/*\"))\n",
        "    self.transform = transform\n",
        "    self.classes, self.class_to_idx = find_classes(targ_dir)\n",
        "\n",
        "  def load_image(self, index: int) -> Image.Image:\n",
        "    '''\n",
        "    Opens an image via a path and returns it.\n",
        "    '''\n",
        "    image_path = self.paths[index]\n",
        "    return Image.open(image_path)\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    '''\n",
        "    Returns the total number of samples.\n",
        "    '''\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[Image.Image, int]:#or Tuple[torch.Tensor, int] with appropriate transform\n",
        "    '''\n",
        "    Returns one sample of data, data and label (X, y).\n",
        "    '''\n",
        "    img = self.load_image(index)\n",
        "    class_name  = self.paths[index].parent.name\n",
        "    class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "    # Transform if necessary\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx\n",
        "    else:\n",
        "      return img, class_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQgpNLtrDHqL"
      },
      "source": [
        "### Let's define transform for our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scq0mZDBoJ9a"
      },
      "outputs": [],
      "source": [
        "!pip install astropy &> /dev/null\n",
        "!pip install photutils &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILyKkW177YXD"
      },
      "outputs": [],
      "source": [
        "from photutils.background import Background2D, MedianBackground\n",
        "import cv2\n",
        "from astropy.convolution import convolve\n",
        "from photutils.segmentation import make_2dgaussian_kernel\n",
        "from photutils.segmentation import detect_sources\n",
        "from photutils.segmentation import deblend_sources\n",
        "\n",
        "class CreateSegmap(object):\n",
        "  def __init__(self, blended:bool=False):\n",
        "    self.blended = blended\n",
        "\n",
        "  def __call__(self, original_img):\n",
        "    #img = np.array(original_img.convert(\"L\")).astype(\"float64\")\n",
        "    img = original_img.convert(\"L\")\n",
        "    img = np.array(img)\n",
        "    img = img.astype(\"float64\")\n",
        "\n",
        "    #removing background noise\n",
        "    bkg_estimator = MedianBackground()\n",
        "    bkg = Background2D(img, (40, 40), filter_size=(7, 7), #The box size should generally be larger than the typical size of sources\n",
        "                                                          #in the image, but small enough to encapsulate any background variations. For best results, the box size should also\n",
        "                                                          # be chosen so that the data are covered by an integer number of boxes in both dimensions.\n",
        "                      bkg_estimator=bkg_estimator, exclude_percentile=20.0)\n",
        "    img -= bkg.background  # subtract the background\n",
        "\n",
        "    #Convolvong with 2D kernel\n",
        "    threshold = 0.5 * bkg.background_rms\n",
        "    kernel = make_2dgaussian_kernel(3.0, size=5)  # FWHM = 3.0\n",
        "    convolved_data = convolve(img, kernel)\n",
        "\n",
        "    #Creating segmap\n",
        "    segment_map = detect_sources(convolved_data, threshold, npixels=10)\n",
        "\n",
        "    #Deblending sources from segmap if deblend==True\n",
        "    if not self.blended:\n",
        "      candidates = self.find_candidates(segment_map)\n",
        "      if len(candidates) != 0:\n",
        "        mask = np.array(segment_map.data == segment_map.labels[candidates[0]])\n",
        "      else:\n",
        "        mask = np.ones((segment_map.data.shape[0], segment_map.data.shape[1]))\n",
        "    else:\n",
        "      segm_deblend = deblend_sources(convolved_data, segment_map,\n",
        "                                npixels=10, nlevels=32, contrast=0.001,\n",
        "                                progress_bar=False)\n",
        "      candidates = self.find_candidates(segm_deblend)\n",
        "      if len(candidates) != 0:\n",
        "        mask = np.array(segm_deblend.data == segm_deblend.labels[candidates[0]])\n",
        "      else:\n",
        "        mask = np.ones((segm_deblend.data.shape[0], segm_deblend.data.shape[1]))\n",
        "\n",
        "    segmented_image = original_img*np.repeat(mask[:, :, np.newaxis], 3, axis=2)#casting 2D grayscale mask to 3D RGB mask repeating it 3 times, can check like [:,:,0], [:,:,1], [:,:,2]\n",
        "    return Image.fromarray(np.uint8(segmented_image))\n",
        "\n",
        "  def find_candidates(self, map):\n",
        "    center = np.array((map.data.shape[0] // 2, map.data.shape[1] // 2))\n",
        "    candidates = []\n",
        "    for n, slc in enumerate(map.slices):\n",
        "      x, y = slc[0], slc[1]\n",
        "      slc_center = np.array([(x.start + x.stop)//2, (y.start + y.stop)//2])\n",
        "      metric = ((slc_center[0] - center[0])**2 + (slc_center[1] - center[1])**2)**(1/2)\n",
        "      if metric < 0.05*map.data.shape[1]:\n",
        "        candidates.append(n)\n",
        "\n",
        "    return candidates\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Creates segmentation map, cropping central source\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bamJQzpPtax1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "For this purpose we should know the minimal size of images provided to rescale them all to it.\n",
        "'''\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "min_size = 1000\n",
        "for image_path in image_path_list:\n",
        "  img = Image.open(image_path)\n",
        "  if img.height < min_size:\n",
        "    min_size = img.height\n",
        "  #if img.height != img.width: print(\"Not a square!\") all the images are squares\n",
        "print(f\"Minimal size is {min_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4FO1n0etczK"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "#So, cause minimal size is 80x80, we should resize all the images to that size\n",
        "\n",
        "augment_transforms = transforms.Compose([\n",
        "  #CreateSegmap(blended=False),\n",
        "  transforms.Resize((80, 80)),#More resolution is not suitable cause minimal size on train set is 80x80\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  transforms.RandomVerticalFlip(p=0.5),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "general_transforms = transforms.Compose([\n",
        "    CreateSegmap(blended=False), #Segmantation stabilising training process and loss function steps, also helps in generalisation?\n",
        "    transforms.Resize((80, 80), interpolation=transforms.InterpolationMode.BICUBIC),#More resolution is not suitable cause minimal size on train set is 80x80\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Grayscale()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5EkPRMtSPt"
      },
      "source": [
        "### Custom Transform visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_dBT2fACWdA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "#random.seed(42)\n",
        "\n",
        "image_path_list = list(test_dir.glob(\"*/*.jpg\"))\n",
        "\n",
        "'''\n",
        "Let's visualise random image\n",
        "'''\n",
        "random_image_path = random.choice(image_path_list)\n",
        "image_class = random_image_path.parent.stem\n",
        "original_img = Image.open(random_image_path)\n",
        "original_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MDpysBECcWD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "segmap = CreateSegmap(blended=False)\n",
        "plt.imshow(segmap(original_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRQxri1Dwvs"
      },
      "source": [
        "### Creating Datasets from our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh1dFziVDv6u"
      },
      "outputs": [],
      "source": [
        "#train_data = CustomDataset(targ_dir=train_dir, transform=augment_transforms)\n",
        "train_data = CustomDataset(targ_dir=train_dir, transform=general_transforms)\n",
        "valid_data = CustomDataset(targ_dir=valid_dir, transform=general_transforms)\n",
        "test_data = CustomDataset(targ_dir=test_dir, transform=general_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS9w1ZuSETOZ"
      },
      "outputs": [],
      "source": [
        "len(train_data),len(valid_data), len(test_data), train_data.classes, valid_data.classes, test_data.classes, train_data.class_to_idx, len(train_data) + len(valid_data) + len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCWdyo9VR4Lr"
      },
      "source": [
        "### Creating Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhfAdUIIR3WY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=torch.cuda.device_count()*2,\n",
        "                              shuffle=True,\n",
        "                              pin_memory=True) #should always be True for training on GPU, speeds up the transfer to it\n",
        "\n",
        "valid_dataloader = DataLoader(dataset=valid_data,\n",
        "                              batch_size=1,\n",
        "                              num_workers=torch.cuda.device_count()*2,\n",
        "                              shuffle=False,\n",
        "                              pin_memory=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                            batch_size=1,\n",
        "                            num_workers=torch.cuda.device_count()*2,\n",
        "                            shuffle=False,\n",
        "                            pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Xs5HNJSktr"
      },
      "outputs": [],
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC70xvhiKtns"
      },
      "source": [
        "### Visualising tensors from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPS7v3UXLCG9"
      },
      "outputs": [],
      "source": [
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool = True,\n",
        "                          seed: int = None):\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "\n",
        "  random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
        "  plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for i, targ_sample in enumerate(random_samples_idx):\n",
        "    targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "    # Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "    targ_image_adjust = targ_image.permute(1, 2, 0)\n",
        "    plt.subplot(1, n, i+1)\n",
        "    plt.imshow(targ_image_adjust)\n",
        "    plt.axis(\"off\")\n",
        "    if classes:\n",
        "      title = f\"class: {classes[targ_label]}\"\n",
        "      if display_shape:\n",
        "        title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
        "    plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmv9AGExLoTU"
      },
      "outputs": [],
      "source": [
        "display_random_images(test_data, n=5, classes=test_data.classes, seed=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1JmV66jB2C7"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  '''\n",
        "  Main bulding block of the net. Consists of two consecutive convolutional\n",
        "  2D layers with ReLU activation and dp% dropout.\n",
        "  '''\n",
        "  def __init__(self, in_channels, out_channels, dp):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(p=dp),\n",
        "      nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),#, bias=False), -> if Normalisation exists\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(p=dp),\n",
        "      nn.MaxPool2d(kernel_size = 2, stride = 2) #The pooling layer summarises the features\n",
        "                                                #present in a region of the feature map generated by a\n",
        "                                                #convolution layer. So, further operations are performed on\n",
        "                                                #summarised features instead of precisely positioned features generated by the\n",
        "                                                #convolution layer. This makes the model more robust to\n",
        "                                                #variations in the position of the features in the input image.\n",
        "    )\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    return self.conv(x)\n",
        "\n",
        "class PRGClassificator(nn.Module):\n",
        "  def __init__(self, input_features: int, output_features: int, hidden_units: int, num_blocks: int, dp: float):\n",
        "    super().__init__()\n",
        "    self.forward_path_list = nn.ModuleList()\n",
        "\n",
        "    for block in range(num_blocks):\n",
        "      self.forward_path_list.append(DoubleConv(input_features, hidden_units, dp))\n",
        "      input_features = hidden_units\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(int(hidden_units*((80*(1/2**num_blocks))**2)), output_features),#As image size is decreased to NxN\n",
        "                                                                        #due to num_blocks pooling layers,\n",
        "                                                                        #where N = 80*(1/2)**(num_blocks)\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    for block in self.forward_path_list:\n",
        "      x = block(x)\n",
        "    return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiIw-JrIXtNv"
      },
      "source": [
        "### Model analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OibykpQmPER"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zypxpanQT2ej"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "#Model initialisation\n",
        "torch.manual_seed(42)\n",
        "model = PRGClassificator(input_features=3, # number of color channels\n",
        "                  output_features=len(train_data.classes),\n",
        "                  hidden_units=10,\n",
        "                  num_blocks = 1,\n",
        "                  dp = 0.5).to(device)\n",
        "\n",
        "summary(model, input_size=[16, 3, 80, 80]) # do a test pass through of an example input size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview &> /dev/null"
      ],
      "metadata": {
        "id": "X6W4AxxUv47E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "graphviz.set_jupyter_format('svg')"
      ],
      "metadata": {
        "id": "xHp1L8vev6b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "\n",
        "# device='meta' -> no memory is consumed for visualization\n",
        "#model_graph = draw_graph(model, input_size=(900, input_features), device='meta', save_graph=True)\n",
        "model_graph = draw_graph(model, input_size=[16, 3, 80, 80], device='meta', save_graph=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "U64wGNvJv8ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgzV8zZhYrht"
      },
      "source": [
        "### Defining train_step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKdhGKA6Yuiv"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer):\n",
        "\n",
        "  #import time\n",
        "\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "  #start_time = 0\n",
        "  for X, y in dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # FORWARD PASS\n",
        "    '''\n",
        "    For multiclass\n",
        "\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "    '''\n",
        "\n",
        "    # FORWARD PASS\n",
        "    y_pred = model(X).view(y.shape)\n",
        "    loss = loss_fn(y_pred, y.float())\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred_class = y_pred.round()\n",
        "    train_acc += ((y_pred_class == y).sum().item()/len(y_pred_class))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Em4LmIQZVGN"
      },
      "source": [
        "### Defining valid_step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHVccXJdZYxP"
      },
      "outputs": [],
      "source": [
        "def valid_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "\n",
        "  model.eval()\n",
        "  valid_loss, valid_acc = 0, 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      '''\n",
        "      Multiclass\n",
        "      # FORWARD PASS\n",
        "      valid_pred_logits = model(X)\n",
        "      loss = loss_fn(valid_pred_logits, y)\n",
        "      valid_loss += loss.item()\n",
        "      valid_pred_labels = valid_pred_logits.argmax(dim=1)\n",
        "      valid_acc += ((valid_pred_labels == y).sum().item()/len(valid_pred_labels))\n",
        "      '''\n",
        "      # FORWARD PASS\n",
        "      valid_pred_logits = model(X).view(y.shape)\n",
        "      loss = loss_fn(valid_pred_logits, y.float())\n",
        "      valid_loss += loss.item()\n",
        "      valid_pred_class = valid_pred_logits.round()\n",
        "      valid_acc += ((valid_pred_class == y).sum().item()/len(valid_pred_class))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  valid_loss = valid_loss / len(dataloader)\n",
        "  valid_acc = valid_acc / len(dataloader)\n",
        "  return valid_loss, valid_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI3DdBhGaCGd"
      },
      "source": [
        "### Defining whole training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW9we71PaHYi"
      },
      "outputs": [],
      "source": [
        "def train(model_name: str,\n",
        "          model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          valid_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 20):\n",
        "\n",
        "  min_val_acc = 0.6\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  from tqdm.auto import tqdm\n",
        "\n",
        "  results = {\"model_name\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"valid_loss\": [],\n",
        "    \"valid_acc\": []\n",
        "  }\n",
        "  print(model_name)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                        dataloader=train_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        optimizer=optimizer)\n",
        "    valid_loss, valid_acc = valid_step(model=model,\n",
        "        dataloader=valid_dataloader,\n",
        "        loss_fn=loss_fn)\n",
        "\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    print(\n",
        "      f\"Epoch: {epoch+1} | \"\n",
        "      f\"train_loss: {train_loss:.4f} | \"\n",
        "      f\"train_acc: {train_acc:.4f} | \"\n",
        "      f\"valid_loss: {valid_loss:.4f} | \"\n",
        "      f\"valid_acc: {valid_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    if valid_acc >= min_val_acc:\n",
        "      torch.save(model.state_dict(), model_name)\n",
        "      print(f\"Saved accuracy is {valid_acc}\")\n",
        "      min_val_acc = valid_acc\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"valid_loss\"].append(valid_loss)\n",
        "    results[\"valid_acc\"].append(valid_acc)\n",
        "\n",
        "  results[\"model_name\"].append(model_name)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc-t-DSKbHm-"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiGlgmXLtPTo"
      },
      "outputs": [],
      "source": [
        "num_blocks = [1]#[1, 2, 3]#4 blocks tested, stable 50%\n",
        "hidden_units = [10]#[10, 50, 100]#[10, 50, 100, 200, 400]\n",
        "learning_rates = [1e-4]#[1e-3, 1e-4]#[1e-3, 5e-4, 1e-4, 5e-5]\n",
        "dropouts = [0.5]#[0.1, 0.2]\n",
        "hyperparams_grid = []\n",
        "for block in num_blocks:\n",
        "  for hu in hidden_units:\n",
        "    for lr in learning_rates:\n",
        "      for dropout in dropouts:\n",
        "        hyperparams_grid.append((block, hu, lr, dropout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlZhSNukaN0J"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "CHECK THESE\n",
        "\n",
        "For ['model_hu10_blocks1_lr0.001'] best valid_loss is 0.6476480677723885 for acc 0.75\n",
        "For ['model_hu10_blocks1_lr0.0005'] best valid_loss is 0.5645014506648295 for acc 0.7     -\n",
        "For ['model_hu50_blocks1_lr0.001'] best valid_loss is 0.6493203544799598 for acc 0.75\n",
        "For ['model_hu100_blocks1_lr0.001'] best valid_loss is 0.6723072484135628 for acc 0.75\n",
        "For ['model_hu100_blocks1_lr0.0005'] best valid_loss is 0.4840030772611385 for acc 0.7    -\n",
        "For ['model_hu200_blocks1_lr0.0005'] best valid_loss is 0.667924758605659 for acc 0.75\n",
        "For ['model_hu200_blocks1_lr0.0001'] best valid_loss is 0.6068458913825452 for acc 0.7\n",
        "For ['model_hu200_blocks1_lr5e-05'] best valid_loss is 0.6206109322607517 for acc 0.75    -\n",
        "For ['model_hu400_blocks1_lr0.0005'] best valid_loss is 0.6931484043598175 for acc 0.7    -\n",
        "For ['model_hu10_blocks2_lr0.0005'] best valid_loss is 0.6179103586357086 for acc 0.7     -\n",
        "For ['model_hu200_blocks2_lr0.0001'] best valid_loss is 0.6879961729049683 for acc 0.75\n",
        "For ['model_hu400_blocks2_lr0.0001'] best valid_loss is 0.6551367722451686 for acc 0.7    -\n",
        "For ['model_hu100_blocks3_lr0.0005'] best valid_loss is 0.6941133096814156 for acc 0.75\n",
        "\n",
        "BEST: model_hu10_blocks1_lr0.0001_dp0.25, model_hu50_blocks1_lr0.0001_dp0.2\n",
        "'''\n",
        "\n",
        "hyperparams_grid = [(1, 50, 0.0001, 0.1)]#[(1, 50, 0.0001, 0.2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4t3plM2bSMK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "grid_results = []\n",
        "for params in hyperparams_grid:\n",
        "  model_results = {}\n",
        "  num_blocks = params[0]\n",
        "  hidden_units = params[1]\n",
        "  lr = params[2]\n",
        "  dropout = params[3]\n",
        "  #Model initialisation\n",
        "  model = PRGClassificator(input_features=3, # number of color channels\n",
        "                          hidden_units=hidden_units,\n",
        "                          output_features=1,#len(train_data.classes),\n",
        "                          num_blocks = num_blocks,\n",
        "                          dp = dropout).to(device)\n",
        "\n",
        "  # Choosing Loss function and optimizer\n",
        "  #loss_fn = nn.CrossEntropyLoss()\n",
        "  loss_fn = nn.BCELoss()\n",
        "  optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr, weight_decay=1e-3)#wd == L2 regularization\n",
        "\n",
        "  # Training model\n",
        "  model_results = train(model_name = f\"model_hu{hidden_units}_blocks{num_blocks}_lr{lr}_dp{dropout}\",\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        valid_dataloader=valid_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)\n",
        "  grid_results.append(model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVTAv6nuA6tK"
      },
      "outputs": [],
      "source": [
        "for result in grid_results:\n",
        "  if max(result['valid_acc']) >= 0.7:\n",
        "    print(f\"For {result['model_name']} best valid_loss is {min(result['valid_loss'])} for acc {max(result['valid_acc'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdscSK231GD1"
      },
      "source": [
        "### Analysing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1n2Dn801KQd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = model_results['train_loss']\n",
        "valid_loss = model_results['valid_loss']\n",
        "accuracy = model_results['train_acc']\n",
        "valid_accuracy = model_results['valid_acc']\n",
        "epochs = range(len(model_results['train_loss']))\n",
        "\n",
        "# Setup a plot\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, label='train_loss')\n",
        "plt.plot(epochs, valid_loss, label='validation_loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "plt.plot(epochs, valid_accuracy, label='validation_accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oes30_bV2KUW"
      },
      "outputs": [],
      "source": [
        "print(loss[90:], valid_loss[90:])\n",
        "print(accuracy[90:], valid_accuracy[90:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making predictions on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHRmpvVP3cEF"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = 0, 0\n",
        "\n",
        "model = PRGClassificator(input_features=3, # number of color channels\n",
        "                          hidden_units=10,\n",
        "                          output_features=1,\n",
        "                          num_blocks = 1,\n",
        "                          dp = 0.5).to(device)\n",
        "model.load_state_dict(torch.load(\"model_hu10_blocks1_lr0.0001_dp0.5\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "test_labels, pred_labels = [], []\n",
        "with torch.inference_mode():\n",
        "  fp = 0\n",
        "  for batch, (X, y) in enumerate(test_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    test_pred_logits = model(X)\n",
        "    loss = loss_fn(test_pred_logits.view(y.shape), y.float())\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    test_pred_labels = test_pred_logits.round()\n",
        "    if test_pred_labels != y:\n",
        "      #print(f\"Model named {test_data.classes[y]} galaxy as {test_data.classes[test_pred_labels]} galaxy!\")\n",
        "      if y == 0:\n",
        "        fp += 1\n",
        "        print(f\"Galaxy classified as prg is {test_data.paths[batch]}\")\n",
        "    test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "    test_labels.append(y.item())\n",
        "    pred_labels.append(test_pred_labels.item())\n",
        "\n",
        "# Adjust metrics to get average loss and accuracy per batch\n",
        "test_loss = test_loss / len(test_dataloader)\n",
        "test_acc = test_acc / len(test_dataloader)\n",
        "\n",
        "print(test_loss, test_acc, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNCnJJtUD9GI"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics &> /dev/null\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLLlWIYFECIL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "confmat = ConfusionMatrix(num_classes=len(test_data.classes), task='multiclass')\n",
        "confmat_tensor = confmat(preds = torch.Tensor(pred_labels), target = torch.Tensor(test_labels))\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    class_names=list(test_data.class_to_idx.keys()),\n",
        "    figsize=(10, 7)\n",
        ")\n",
        "\n",
        "plt.savefig(\"conf_matrix.pdf\", dpi=400, format='pdf', bbox_inches='tight')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oxQxCBWyoGPE",
        "kNgm0bq3QleS",
        "9MbxNF_A4yy4",
        "EHHycu_f9Ook",
        "LcnjWcaWBUgx",
        "EQgpNLtrDHqL",
        "GG5EkPRMtSPt",
        "sfRQxri1Dwvs",
        "VCWdyo9VR4Lr",
        "MC70xvhiKtns",
        "af8O4l90gk7B",
        "FiIw-JrIXtNv",
        "D6XkI90snSDl",
        "U3PZasO0006Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}